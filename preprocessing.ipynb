{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMIzl827bzza9EGlY2Oz0x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanmayAmte/FYP/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXq4copBE888"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mediapipe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPuJ2KrAE-NH",
        "outputId": "3bac1c82-caa9-45f9-ecab-8f7a6abc543f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.21)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.1)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdxigVnaFi9M",
        "outputId": "aea2b3f6-1414-4df3-bfd7-6b494c4abe54"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=0bc7c1304b1ca1de603262de0f0bf7b9a34fafb47e1599db04d2d6fc60da8c82\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CShawSfZen0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86434391-eea1-4ffa-de56-03c74291e277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Audio from: /content/clip1.mp4...\n",
            "Audio extracted successfully.\n",
            "Audio Data Shape: (5404896,), Sample Rate: 44100\n",
            "Transcribing audio using Whisper (base model)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription complete.\n",
            "Transcript:  effective speaking in spontaneous situations. I thank you all for joining us, even though the title...\n",
            "Processing video frames for landmarks...\n",
            "Video processing complete. Analyzed 368 frames.\n",
            "First frame data keys: dict_keys(['frame_index', 'timestamp', 'face_landmarks', 'pose_landmarks'])\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import moviepy.editor as mp\n",
        "import librosa\n",
        "import numpy as np\n",
        "import mediapipe as mp_solutions\n",
        "import whisper\n",
        "import os\n",
        "\n",
        "# Initialize MediaPipe solutions\n",
        "mp_face_mesh = mp_solutions.solutions.face_mesh\n",
        "mp_pose = mp_solutions.solutions.pose\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, video_path):\n",
        "        self.video_path = video_path\n",
        "        self.audio_path = \"temp_audio.wav\"\n",
        "        self.video_capture = cv2.VideoCapture(video_path)\n",
        "\n",
        "        # Video valid check\n",
        "        if not self.video_capture.isOpened():\n",
        "            raise ValueError(\"Error opening video file\")\n",
        "\n",
        "    def extract_audio(self):\n",
        "        print(f\"Processing Audio from: {self.video_path}...\")\n",
        "        try:\n",
        "            video_clip = mp.VideoFileClip(self.video_path)\n",
        "            # Write audio to a temporary file (16-bit PCM WAV is standard for ML)\n",
        "\n",
        "            video_clip.audio.write_audiofile(self.audio_path, verbose=False, logger=None)\n",
        "            print(\"Audio extracted successfully.\")\n",
        "            return self.audio_path\n",
        "        except Exception as e:\n",
        "            print(f\"Audio extraction failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_audio_data(self):\n",
        "        \"\"\"\n",
        "        Loads the extracted audio file into a numpy array for analysis.\n",
        "        Returns:\n",
        "            y: Audio time series\n",
        "            sr: Sampling rate\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.audio_path):\n",
        "            print(\"Audio file not found. Run extract_audio() first.\")\n",
        "            return None, None\n",
        "\n",
        "        # Librosa loads audio as a float array. sr=None preserves original sampling rate.\n",
        "        y, sr = librosa.load(self.audio_path, sr=None)\n",
        "        return y, sr\n",
        "\n",
        "    def transcribe_audio(self, model_size=\"base\"):\n",
        "        \"\"\"\n",
        "        Uses OpenAI Whisper to transcribe audio with timestamps.\n",
        "        Returns : dictionary containing the full text and word-level segments.\n",
        "        \"\"\"\n",
        "        print(f\"Transcribing audio using Whisper ({model_size} model)...\")\n",
        "        model = whisper.load_model(model_size)\n",
        "\n",
        "        # Transcribe with word_timestamps=True for pace analysis later\n",
        "        result = model.transcribe(self.audio_path, word_timestamps=True)\n",
        "\n",
        "        print(\"Transcription complete.\")\n",
        "        return {\n",
        "            \"text\": result[\"text\"],\n",
        "            \"segments\": result[\"segments\"] # Contains start/end times for words\n",
        "        }\n",
        "\n",
        "    def process_video_frames(self, frame_skip=5):\n",
        "        \"\"\"\n",
        "        Extracts visual landmarks from video frames.\n",
        "\n",
        "        Args:\n",
        "            frame_skip (int): Process every Nth frame to save computation time.\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries containing analysis data for processed frames.\n",
        "        \"\"\"\n",
        "        print(\"Processing video frames for landmarks...\")\n",
        "\n",
        "        frame_data = []\n",
        "        frame_count = 0\n",
        "\n",
        "        # Setup MediaPipe instances\n",
        "        with mp_face_mesh.FaceMesh(\n",
        "            static_image_mode=False,\n",
        "            max_num_faces=1,\n",
        "            refine_landmarks=True,\n",
        "            min_detection_confidence=0.5\n",
        "        ) as face_mesh, \\\n",
        "        mp_pose.Pose(\n",
        "            static_image_mode=False,\n",
        "            min_detection_confidence=0.5\n",
        "        ) as pose_detector:\n",
        "\n",
        "            while True:\n",
        "                ret, frame = self.video_capture.read()\n",
        "                if not ret: #Video end\n",
        "                    break\n",
        "\n",
        "                # Skip frames to optimize speed\n",
        "                if frame_count % frame_skip != 0:\n",
        "                    frame_count += 1\n",
        "                    continue\n",
        "\n",
        "                # Convert BGR (OpenCV default) to RGB (MediaPipe requirement)\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # 1. Detect Face Landmarks\n",
        "                face_results = face_mesh.process(frame_rgb)\n",
        "\n",
        "                # 2. Detect Body Pose\n",
        "                pose_results = pose_detector.process(frame_rgb)\n",
        "\n",
        "                # Store results if detections exist\n",
        "                frame_info = {\n",
        "                    \"frame_index\": frame_count,\n",
        "                    \"timestamp\": self.video_capture.get(cv2.CAP_PROP_POS_MSEC) / 1000.0,\n",
        "                    \"face_landmarks\": None,\n",
        "                    \"pose_landmarks\": None\n",
        "                }\n",
        "\n",
        "                if face_results.multi_face_landmarks:\n",
        "                    # We only take the first face detected\n",
        "                    frame_info[\"face_landmarks\"] = face_results.multi_face_landmarks[0]\n",
        "\n",
        "                if pose_results.pose_landmarks:\n",
        "                    frame_info[\"pose_landmarks\"] = pose_results.pose_landmarks\n",
        "\n",
        "                frame_data.append(frame_info)\n",
        "                frame_count += 1\n",
        "\n",
        "        self.video_capture.release()\n",
        "        print(f\"Video processing complete. Analyzed {len(frame_data)} frames.\")\n",
        "        return frame_data\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Deletes temporary audio files.\"\"\"\n",
        "        if os.path.exists(self.audio_path):\n",
        "            os.remove(self.audio_path)\n",
        "            print(\"Temporary files cleaned up.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual video file path\n",
        "    VIDEO_FILE = \"/content/clip1.mp4\"\n",
        "\n",
        "    # Create a dummy video file if it doesn't exist (for testing purposes)\n",
        "    if not os.path.exists(VIDEO_FILE):\n",
        "        print(f\"Please provide a valid video file at {VIDEO_FILE}\")\n",
        "    else:\n",
        "        processor = Preprocessor(VIDEO_FILE)\n",
        "\n",
        "        # 1. Extract Audio\n",
        "        processor.extract_audio()\n",
        "\n",
        "        # 2. Get Audio Data (for Librosa analysis later)\n",
        "        y, sr = processor.load_audio_data()\n",
        "        print(f\"Audio Data Shape: {y.shape}, Sample Rate: {sr}\")\n",
        "\n",
        "        # 3. Get Transcript (Text Data)\n",
        "        transcript_data = processor.transcribe_audio()\n",
        "        print(f\"Transcript: {transcript_data['text'][:100]}...\") # Print first 100 chars\n",
        "\n",
        "        # 4. Get Visual Landmarks (Video Data)\n",
        "        visual_data = processor.process_video_frames(frame_skip=10) # Process every 10th frame\n",
        "        print(f\"First frame data keys: {visual_data[0].keys()}\")\n",
        "\n",
        "        # Cleanup\n",
        "        #processor.cleanup()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import shutil\n",
        "\n",
        "# Save Text Transcript\n",
        "with open(\"transcript.txt\", \"w\") as f:\n",
        "    f.write(transcript_data['text'])\n",
        "\n",
        "# Save Audio File\n",
        "shutil.copy(processor.audio_path, \"audio.wav\")\n",
        "\n",
        "# Save Visual Data (Convert MediaPipe objects to JSON format)\n",
        "clean_visual_data = []\n",
        "for frame in visual_data:\n",
        "    clean_visual_data.append({\n",
        "        \"timestamp\": frame[\"timestamp\"],\n",
        "\n",
        "        \"face\": [{\"x\": lm.x, \"y\": lm.y, \"z\": lm.z} for lm in frame[\"face_landmarks\"].landmark] if frame[\"face_landmarks\"] else [],\n",
        "\n",
        "        \"pose\": [{\"x\": lm.x, \"y\": lm.y, \"z\": lm.z} for lm in frame[\"pose_landmarks\"].landmark] if frame[\"pose_landmarks\"] else []\n",
        "    })\n",
        "\n",
        "with open(\"visual_data.json\", \"w\") as f:\n",
        "    json.dump(clean_visual_data, f)\n",
        "\n",
        "print(\"Saved: transcript.txt, audio.wav, visual_data.json\")\n",
        "\n",
        "# Only if using Google Colab, run this to download to your PC:\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(\"transcript.txt\")\n",
        "    files.download(\"audio.wav\")\n",
        "    files.download(\"visual_data.json\")\n",
        "except ImportError:\n",
        "    pass"
      ],
      "metadata": {
        "id": "LFPNEJeXZqp4",
        "outputId": "4c87f96c-0d38-4a28-c412-daf952a49d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: transcript.txt, audio.wav, visual_data.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e6b484f2-365f-4ebb-866a-55a473d37eb0\", \"transcript.txt\", 1911)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_17f59349-c32c-48e4-a506-c1a3ae578c07\", \"audio.wav\", 21619662)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_eb60da58-1f22-4804-ae70-b25c1e513622\", \"visual_data.json\", 4073581)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}