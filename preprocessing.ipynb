{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEeApuOC5UssST0SrFPl8A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanmayAmte/FYP/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CShawSfZen0w"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import moviepy.editor as mp\n",
        "import librosa\n",
        "import numpy as np\n",
        "import mediapipe as mp_solutions\n",
        "import whisper\n",
        "import os\n",
        "\n",
        "# Initialize MediaPipe solutions\n",
        "mp_face_mesh = mp_solutions.solutions.face_mesh\n",
        "mp_pose = mp_solutions.solutions.pose\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, video_path):\n",
        "        self.video_path = video_path\n",
        "        self.audio_path = \"temp_audio.wav\"\n",
        "        self.video_capture = cv2.VideoCapture(video_path)\n",
        "\n",
        "        # Check if video loaded correctly\n",
        "        if not self.video_capture.isOpened():\n",
        "            raise ValueError(\"Error opening video file\")\n",
        "\n",
        "    def extract_audio(self):\n",
        "        \"\"\"\n",
        "        Extracts audio from video and saves it as a temporary WAV file.\n",
        "        Returns the path to the audio file.\n",
        "        \"\"\"\n",
        "        print(f\"Processing Audio from: {self.video_path}...\")\n",
        "        try:\n",
        "            video_clip = mp.VideoFileClip(self.video_path)\n",
        "            # Write audio to a temporary file (16-bit PCM WAV is standard for ML)\n",
        "            video_clip.audio.write_audiofile(self.audio_path, verbose=False, logger=None)\n",
        "            print(\"Audio extracted successfully.\")\n",
        "            return self.audio_path\n",
        "        except Exception as e:\n",
        "            print(f\"Audio extraction failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_audio_data(self):\n",
        "        \"\"\"\n",
        "        Loads the extracted audio file into a numpy array for analysis.\n",
        "        Returns:\n",
        "            y: Audio time series\n",
        "            sr: Sampling rate\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.audio_path):\n",
        "            print(\"Audio file not found. Run extract_audio() first.\")\n",
        "            return None, None\n",
        "\n",
        "        # Librosa loads audio as a float array. sr=None preserves original sampling rate.\n",
        "        y, sr = librosa.load(self.audio_path, sr=None)\n",
        "        return y, sr\n",
        "\n",
        "    def transcribe_audio(self, model_size=\"base\"):\n",
        "        \"\"\"\n",
        "        Uses OpenAI Whisper to transcribe audio with timestamps.\n",
        "        Returns a dictionary containing the full text and word-level segments.\n",
        "        \"\"\"\n",
        "        print(f\"Transcribing audio using Whisper ({model_size} model)...\")\n",
        "        model = whisper.load_model(model_size)\n",
        "\n",
        "        # Transcribe with word_timestamps=True for pace analysis later\n",
        "        result = model.transcribe(self.audio_path, word_timestamps=True)\n",
        "\n",
        "        print(\"Transcription complete.\")\n",
        "        return {\n",
        "            \"text\": result[\"text\"],\n",
        "            \"segments\": result[\"segments\"] # Contains start/end times for words\n",
        "        }\n",
        "\n",
        "    def process_video_frames(self, frame_skip=5):\n",
        "        \"\"\"\n",
        "        Extracts visual landmarks from video frames.\n",
        "\n",
        "        Args:\n",
        "            frame_skip (int): Process every Nth frame to save computation time.\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries containing analysis data for processed frames.\n",
        "        \"\"\"\n",
        "        print(\"Processing video frames for landmarks...\")\n",
        "\n",
        "        frame_data = []\n",
        "        frame_count = 0\n",
        "\n",
        "        # Setup MediaPipe instances\n",
        "        with mp_face_mesh.FaceMesh(\n",
        "            static_image_mode=False,\n",
        "            max_num_faces=1,\n",
        "            refine_landmarks=True, # Essential for eye gaze tracking\n",
        "            min_detection_confidence=0.5\n",
        "        ) as face_mesh, \\\n",
        "        mp_pose.Pose(\n",
        "            static_image_mode=False,\n",
        "            min_detection_confidence=0.5\n",
        "        ) as pose_detector:\n",
        "\n",
        "            while True:\n",
        "                ret, frame = self.video_capture.read()\n",
        "                if not ret:\n",
        "                    break # End of video\n",
        "\n",
        "                # Skip frames to optimize speed (processing 30FPS is usually unnecessary)\n",
        "                if frame_count % frame_skip != 0:\n",
        "                    frame_count += 1\n",
        "                    continue\n",
        "\n",
        "                # Convert BGR (OpenCV default) to RGB (MediaPipe requirement)\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # 1. Detect Face Landmarks\n",
        "                face_results = face_mesh.process(frame_rgb)\n",
        "\n",
        "                # 2. Detect Body Pose\n",
        "                pose_results = pose_detector.process(frame_rgb)\n",
        "\n",
        "                # Store results if detections exist\n",
        "                frame_info = {\n",
        "                    \"frame_index\": frame_count,\n",
        "                    \"timestamp\": self.video_capture.get(cv2.CAP_PROP_POS_MSEC) / 1000.0,\n",
        "                    \"face_landmarks\": None,\n",
        "                    \"pose_landmarks\": None\n",
        "                }\n",
        "\n",
        "                if face_results.multi_face_landmarks:\n",
        "                    # We only take the first face detected\n",
        "                    frame_info[\"face_landmarks\"] = face_results.multi_face_landmarks[0]\n",
        "\n",
        "                if pose_results.pose_landmarks:\n",
        "                    frame_info[\"pose_landmarks\"] = pose_results.pose_landmarks\n",
        "\n",
        "                frame_data.append(frame_info)\n",
        "                frame_count += 1\n",
        "\n",
        "        self.video_capture.release()\n",
        "        print(f\"Video processing complete. Analyzed {len(frame_data)} frames.\")\n",
        "        return frame_data\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Deletes temporary audio files.\"\"\"\n",
        "        if os.path.exists(self.audio_path):\n",
        "            os.remove(self.audio_path)\n",
        "            print(\"Temporary files cleaned up.\")\n",
        "\n",
        "# =========================================\n",
        "# Example Usage\n",
        "# =========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual video file path\n",
        "    VIDEO_FILE = \"input_video.mp4\"\n",
        "\n",
        "    # Create a dummy video file if it doesn't exist (for testing purposes)\n",
        "    if not os.path.exists(VIDEO_FILE):\n",
        "        print(f\"Please provide a valid video file at {VIDEO_FILE}\")\n",
        "    else:\n",
        "        processor = Preprocessor(VIDEO_FILE)\n",
        "\n",
        "        # 1. Extract Audio\n",
        "        processor.extract_audio()\n",
        "\n",
        "        # 2. Get Audio Data (for Librosa analysis later)\n",
        "        y, sr = processor.load_audio_data()\n",
        "        print(f\"Audio Data Shape: {y.shape}, Sample Rate: {sr}\")\n",
        "\n",
        "        # 3. Get Transcript (Text Data)\n",
        "        transcript_data = processor.transcribe_audio()\n",
        "        print(f\"Transcript: {transcript_data['text'][:100]}...\") # Print first 100 chars\n",
        "\n",
        "        # 4. Get Visual Landmarks (Video Data)\n",
        "        visual_data = processor.process_video_frames(frame_skip=10) # Process every 10th frame\n",
        "        print(f\"First frame data keys: {visual_data[0].keys()}\")\n",
        "\n",
        "        # Cleanup\n",
        "        processor.cleanup()"
      ]
    }
  ]
}